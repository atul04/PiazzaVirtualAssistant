{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. embedding [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. embedding\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', 'em', '##bed', '##ding', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('after', 2044)\n",
      "('stealing', 11065)\n",
      "('money', 2769)\n",
      "('from', 2013)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('vault', 11632)\n",
      "(',', 1010)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('robber', 27307)\n",
      "('was', 2001)\n",
      "('seen', 2464)\n",
      "('fishing', 5645)\n",
      "('on', 2006)\n",
      "('the', 1996)\n",
      "('mississippi', 5900)\n",
      "('river', 2314)\n",
      "('bank', 2924)\n",
      "('.', 1012)\n",
      "('em', 7861)\n",
      "('##bed', 8270)\n",
      "('##ding', 4667)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "  print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1646e-02, -2.7428e-01, -4.2455e-02,  3.5630e-01,  7.8327e-01,\n",
       "          2.1895e-01, -5.0150e-02,  4.8984e-01,  6.0959e-02, -3.9689e-01,\n",
       "          6.3904e-02, -1.1114e-01,  2.5166e-02,  3.6832e-01, -4.7702e-01,\n",
       "          1.8981e-01,  2.5177e-01,  8.4108e-02,  4.9402e-01,  1.2160e-01,\n",
       "          1.0485e-01, -8.8513e-03,  1.6972e-01,  2.1172e-01,  3.5628e-01,\n",
       "          1.1330e-01, -2.2521e-01,  1.0850e-01, -2.2228e-01,  2.6852e-01,\n",
       "          5.1576e-01, -1.1650e-01,  1.5095e-01, -3.9875e-01, -1.3337e-01,\n",
       "         -4.2994e-01, -2.3136e-01, -5.1193e-02, -1.0509e-01,  2.5181e-01,\n",
       "         -3.0920e-01, -3.5429e-01, -2.4497e-01,  2.1342e-01,  5.6274e-02,\n",
       "         -4.7383e-01, -9.8232e-02, -1.3003e-01, -1.7795e-01,  6.7389e-02,\n",
       "         -3.1509e-01,  7.6815e-01, -6.1594e-01, -2.3484e-02,  1.0953e-01,\n",
       "          6.5318e-01, -3.1962e-01, -6.5844e-01, -5.4310e-02, -1.0029e-01,\n",
       "          3.4368e-01, -3.3413e-01,  3.7650e-01, -4.7105e-01,  8.7584e-02,\n",
       "          6.1833e-03,  4.1972e-01,  1.7189e-01, -3.1287e-01,  2.2347e-02,\n",
       "         -3.5401e-01,  2.3494e-01, -1.5566e-01, -4.8569e-01, -5.0042e-02,\n",
       "          3.1581e-01, -1.3727e-01,  5.2178e-02, -2.5753e-01,  2.0502e-01,\n",
       "         -1.3896e-01,  3.1224e-01, -1.5849e-01,  2.4086e-01,  9.0516e-02,\n",
       "          4.9025e-02,  2.0589e-02,  8.1054e-02, -6.2214e-02,  5.4753e-01,\n",
       "         -7.8044e-02,  5.9233e-02, -2.6315e-01,  5.6777e-03, -3.8673e-01,\n",
       "          1.4415e-01, -1.7894e-01,  7.0718e-02, -2.4774e-01,  4.0958e-01,\n",
       "         -4.7111e-02, -5.1000e-01,  1.7965e-01,  3.7998e-01,  2.0480e-02,\n",
       "          2.7902e-02,  2.7470e-01,  2.2288e-01, -2.6848e-01, -6.8985e-02,\n",
       "          1.3882e-01, -2.8467e-01,  2.4286e-01, -1.6129e-01, -9.3578e-03,\n",
       "         -4.8989e-02,  5.0700e-01, -2.9507e-01, -4.8762e-01, -4.8277e-02,\n",
       "          3.9001e-01,  3.0321e-01,  3.8065e-01,  9.2160e-01, -5.8430e-02,\n",
       "          1.8684e-01, -3.1164e-02,  9.8771e-02, -5.3407e-02, -5.3591e-01,\n",
       "          3.4570e-01,  2.9832e-02, -3.1431e-02, -1.3005e-01, -2.2007e-01,\n",
       "          2.2746e-01, -8.9087e-02, -2.5143e-01, -3.9925e-01,  2.2390e-01,\n",
       "          1.6739e-01, -2.5522e-01,  2.9284e-03,  1.8745e-01,  1.2434e-01,\n",
       "          3.2096e-01,  1.9578e-01, -2.3533e-01,  2.0619e-01,  2.8698e-01,\n",
       "          2.5212e-01,  2.7015e-01, -2.4198e-01, -2.1793e-01,  2.1776e-01,\n",
       "          2.3250e-01,  2.1671e-02,  4.2377e-02, -1.9232e-01, -2.7690e-02,\n",
       "          3.3266e-01,  3.2735e-01, -3.7334e-01,  2.8837e-01, -9.3109e-02,\n",
       "         -9.4972e-02,  1.2573e-01,  1.6729e-01, -2.0387e-01, -3.9042e-02,\n",
       "         -2.3434e-01, -2.5629e-02,  5.8811e-01,  2.4643e-02,  1.1508e-01,\n",
       "          1.1590e-01, -1.4044e-01, -7.3878e-02,  1.4789e-01, -2.8251e-02,\n",
       "         -1.0575e+00,  2.7311e-01,  2.1568e-01,  1.5133e-01,  2.6989e-01,\n",
       "         -1.9430e-01,  3.1738e-01,  1.3396e-01, -1.9212e-01, -3.1234e-01,\n",
       "          7.6941e-02, -5.1352e-01, -6.3638e-01,  9.0510e-02, -4.0561e-02,\n",
       "         -9.5713e-02, -3.0011e-01, -1.1051e-01, -7.3314e-02, -5.3624e-02,\n",
       "         -2.6177e-01, -5.4208e-02,  2.0644e-01,  7.6147e-02,  3.4010e-04,\n",
       "          2.3951e-02,  1.8490e-01, -1.9578e-02,  4.1352e-01,  1.7411e-01,\n",
       "         -2.0257e-01,  3.8622e-01,  1.6076e-01, -4.3518e-02,  1.0301e-01,\n",
       "         -1.3237e-01, -2.6903e-01, -4.7696e-01,  5.1126e-03,  1.1169e-01,\n",
       "         -2.7196e-02,  2.8770e-01, -1.0527e-01,  3.6502e-01,  1.5014e-02,\n",
       "          8.5476e-01, -2.4173e-01, -3.3721e-01, -2.1084e-01,  1.2239e-01,\n",
       "         -1.9063e-01, -1.1702e-01,  2.2121e-01, -9.7392e-02, -2.0538e-01,\n",
       "         -5.0193e-02, -2.0448e-02, -8.1061e-02, -1.7168e-01, -3.3794e-01,\n",
       "         -5.8501e-02,  1.2916e-01,  4.8006e-01, -1.0564e-01,  1.9816e-01,\n",
       "         -3.2302e-02,  2.5870e-01,  5.2942e-01, -1.1898e-01, -2.6245e-01,\n",
       "         -3.7280e-01, -2.4396e-01, -2.9742e-01, -4.2863e-01,  1.9494e-01,\n",
       "         -1.7290e-01, -1.0845e-01, -7.5270e-02,  4.4981e-03,  1.7508e-01,\n",
       "          3.8252e-01, -8.7430e-02, -2.6883e-02, -4.5373e-02, -1.9720e-01,\n",
       "         -3.1361e-01, -3.0977e-01,  2.4946e-01,  3.1379e-01,  2.2730e-01,\n",
       "          8.4108e-02,  1.3018e-01,  4.0233e-02,  4.3390e-01,  1.1528e-01,\n",
       "         -1.8460e-01,  9.4047e-02,  7.8173e-02,  8.8990e-02, -1.2832e-01,\n",
       "          1.3446e-01,  3.8816e-01, -3.1608e-01, -3.2205e-01,  1.1504e-01,\n",
       "         -3.0539e-01,  4.2782e-02,  5.4093e-01, -2.2912e-01, -2.0253e-01,\n",
       "         -4.7722e-02,  1.7305e-01, -3.5210e-01,  2.0325e-01, -1.2559e-01,\n",
       "          2.5857e-01,  3.7477e-03,  6.5955e-01,  5.9500e-01, -4.0141e-02,\n",
       "          3.3596e-03,  9.1899e-02,  3.7765e-01, -3.4921e-02, -9.0647e-02,\n",
       "          4.0187e-02,  4.6837e-01, -2.9089e-01, -4.2177e+00,  6.2771e-01,\n",
       "          3.8050e-01, -3.3698e-01,  1.2023e-01, -1.2394e-01, -2.1404e-01,\n",
       "         -4.2955e-01, -2.7179e-01,  9.1991e-02, -2.3187e-01, -5.7365e-01,\n",
       "          4.3775e-01, -1.3180e-01,  1.0445e-01, -3.9577e-01,  2.4761e-01,\n",
       "         -2.0251e-01,  6.3868e-02,  1.2256e-01, -2.7493e-01, -2.1371e-01,\n",
       "         -2.2803e-01,  8.0862e-02,  4.0949e-01,  3.6303e-01, -6.2829e-01,\n",
       "          5.3444e-02,  1.2802e-01,  6.8184e-02,  1.0805e-01, -2.6069e-01,\n",
       "         -2.0091e-01, -3.7778e-01,  2.0808e-01,  6.4645e-02, -3.1055e-02,\n",
       "         -7.0195e-02,  1.1171e-01,  1.6970e-01, -2.2638e-01, -4.0377e-01,\n",
       "         -1.7460e-01, -2.4194e-02,  9.4511e-01,  1.8343e-01, -5.2386e-02,\n",
       "         -5.5351e-01,  8.7157e-02,  1.8157e-01,  4.1054e-01, -3.7095e-01,\n",
       "         -9.3232e-02,  2.8381e-01,  2.0373e-01, -7.6650e-04,  2.3849e-01,\n",
       "          5.9567e-02, -1.9520e-01, -1.5530e-01, -1.1363e-01, -4.1650e-01,\n",
       "         -2.0237e-01,  1.7195e-01, -5.3862e-02, -3.2323e-01, -3.3564e-01,\n",
       "          3.9400e-02,  2.1783e-01,  6.4307e-02, -5.3462e-02,  3.0733e-01,\n",
       "         -3.8965e-01, -8.5370e-01, -3.3795e-01,  3.9803e-02,  5.5881e-01,\n",
       "         -2.4414e-01,  2.1048e-01, -1.4443e-01, -3.2982e-01, -1.0111e-01,\n",
       "          1.7442e-01,  3.0266e-02,  3.0276e-02, -3.3739e-01,  3.0244e-02,\n",
       "          2.2977e-01, -3.7677e-01, -5.3196e-01,  2.5440e-01,  2.9221e-01,\n",
       "          1.7142e-01, -5.1496e-02,  8.2988e-02,  5.3693e-02, -1.9968e-01,\n",
       "         -2.8214e-01,  5.1436e-02,  1.8023e-02,  1.6014e-01, -1.1766e-01,\n",
       "          3.7324e-01, -2.7330e-01, -6.4651e-02, -1.1297e-01, -2.7177e-01,\n",
       "          7.5423e-02, -5.5792e-01,  2.2879e-01,  2.4242e-01, -6.3162e-01,\n",
       "          6.6050e-01,  2.8139e-02,  1.1126e-01,  2.5066e-01,  5.6591e-01,\n",
       "          3.4117e-01,  1.4173e-01,  4.6219e-02, -3.2003e-01,  5.5092e-01,\n",
       "         -4.1561e-01, -3.6245e-01,  2.2650e-02, -1.2092e-01,  3.5361e-02,\n",
       "         -6.0898e-02,  2.7024e-03, -4.2688e-01,  9.3343e-02, -3.5025e-01,\n",
       "          1.6733e-01,  1.0473e-01,  4.7048e-01, -1.6659e-01,  2.3450e-01,\n",
       "         -9.5543e-01,  7.5668e-02,  2.4800e-01, -1.2310e-01,  3.5721e-03,\n",
       "          9.3809e-02, -1.7120e-01, -2.2369e-01,  1.3177e-01,  2.0100e-02,\n",
       "         -8.7770e-02, -9.1973e-02, -7.6939e-02, -3.2589e-01, -9.7160e-02,\n",
       "          1.3470e-01, -4.3150e-01, -1.5666e-01,  3.7806e-01,  2.3398e-01,\n",
       "         -1.5443e-01, -4.9362e-01, -8.5777e-02,  1.7316e-01,  2.0668e-01,\n",
       "          3.8173e-01,  7.8196e-02, -9.1806e-01,  2.1363e-01, -4.3684e-01,\n",
       "         -2.0484e-01, -2.5964e-01,  5.7678e-01,  1.4474e-02, -2.4837e-01,\n",
       "          3.9330e-02,  1.5871e-01, -2.7883e-02,  3.6258e-01,  1.4813e-01,\n",
       "         -3.2574e-01, -1.1970e-01,  8.1526e-02, -2.8232e-01,  1.2616e-01,\n",
       "         -7.4043e-02, -2.5993e-01,  3.2443e-01, -8.7091e-02, -4.1982e-02,\n",
       "          6.5980e-02,  1.0437e-01,  6.4116e-02,  5.8662e-02,  3.4397e-01,\n",
       "         -3.8440e-02,  1.5037e-01, -3.2674e-01, -2.7289e-01,  1.3577e-01,\n",
       "          2.2445e-01,  2.5487e-02,  3.3640e-01,  1.6794e-01,  1.1338e-02,\n",
       "         -2.6774e-01, -6.9100e-02, -1.0064e-01, -3.2657e-01,  2.5673e-01,\n",
       "          2.8813e-01, -3.4367e-01,  2.7603e-01, -2.2375e-01, -1.2103e-02,\n",
       "         -6.3439e-01, -5.8248e-01,  3.6660e-02, -1.0049e-01,  4.3203e-01,\n",
       "          1.5304e-01, -2.5209e-01, -7.6161e-02, -1.4671e-01, -6.4912e-02,\n",
       "          1.6623e-01, -4.6252e-02, -5.0993e-01, -1.6316e-01,  2.4132e-01,\n",
       "          1.2427e-01,  2.4708e-02,  1.3280e-01,  1.5665e-02, -7.9885e-02,\n",
       "         -4.7467e-03,  1.7861e-01,  3.4154e-01,  1.0195e-01, -1.2637e-01,\n",
       "         -6.3952e-01, -1.9848e-01, -1.8358e-01, -1.0386e-01,  2.5660e-01,\n",
       "         -1.7948e-01, -2.5389e-02,  2.6166e-01,  3.0825e-01,  2.0378e-01,\n",
       "          3.0332e-01,  1.9788e-02,  2.9216e-01,  2.3874e-02, -8.4977e-02,\n",
       "          1.9587e-01,  1.2738e-01, -4.2802e-01, -1.8932e-01, -3.5191e-01,\n",
       "          7.7653e-02, -4.6272e-02,  3.4260e-01,  2.4034e-02, -4.0628e-01,\n",
       "         -3.0562e-01,  2.3529e-01,  4.3604e-01,  3.6333e-02,  1.1093e-01,\n",
       "          3.4855e-01,  6.6475e-02, -1.6496e-01,  1.3166e-01, -4.6289e-01,\n",
       "          7.3864e-02,  1.1152e-02, -2.3522e-01,  2.9151e-01,  8.5543e-03,\n",
       "         -6.8325e-01, -2.4929e-01, -3.9269e-01, -4.0800e-01,  8.9593e-02,\n",
       "          1.5912e-01, -4.6886e-02,  4.5136e-02, -1.1893e-01, -4.4697e-03,\n",
       "          2.2628e-01, -1.1403e-01, -7.3683e-01,  1.6632e-01,  4.0062e-02,\n",
       "          1.9401e-01, -3.4868e-02, -1.7646e-01,  2.5105e-01,  4.7340e-01,\n",
       "          1.6331e-01,  3.0211e-01,  3.3110e-01, -9.8451e-02,  1.0899e-01,\n",
       "         -3.2928e-03,  2.5364e-01, -1.7989e-01,  3.1227e-01, -1.8503e-01,\n",
       "         -2.3737e-01, -1.4714e-01,  1.3387e-01, -7.4007e-02, -6.2870e-01,\n",
       "          3.4302e-01,  6.8974e-02, -3.2966e-01, -2.4581e-01,  2.3443e-02,\n",
       "         -1.6544e-01, -6.8264e-01,  1.4906e-01,  7.2938e-02, -4.3300e-02,\n",
       "          3.5407e-01,  2.7247e-01,  1.9614e-01,  3.7570e-01, -2.1177e-01,\n",
       "          5.3069e-02, -1.0399e-01,  3.1998e-01, -2.6074e-01,  2.7843e-02,\n",
       "         -2.2418e-01,  2.4131e-01, -1.9104e-01, -5.6782e-02,  7.0549e-02,\n",
       "         -4.1843e-01,  2.9671e-01, -2.6210e-01,  2.5369e-02, -1.1147e-01,\n",
       "          3.1548e-02,  7.1527e-01,  2.6561e-01, -2.9342e-02, -4.2071e-02,\n",
       "         -6.7688e-02,  4.0806e-01,  4.3499e-01,  6.5205e-02,  7.6585e-02,\n",
       "         -1.1925e-01, -5.9912e-01,  6.2302e-01,  6.7492e-02,  2.7255e-01,\n",
       "         -6.0694e-02, -3.4032e-01,  3.8022e-01,  2.2814e-01, -2.0414e-01,\n",
       "          4.7198e-01, -4.1908e-01, -1.8191e-01,  1.1152e-01, -3.7447e-01,\n",
       "         -3.1561e-01, -1.4297e-02, -1.1335e-01,  2.6851e-01,  2.8706e-01,\n",
       "         -2.9540e-01, -3.3900e-01, -7.6403e-02,  3.5745e-01,  7.6169e-03,\n",
       "          2.9281e-01, -2.0142e-02, -1.1438e-01, -1.8556e-01, -2.9250e-01,\n",
       "         -4.5004e-01, -4.7123e-01, -2.8028e-01, -6.6309e-02, -3.3889e-02,\n",
       "         -1.4704e-02, -1.0398e-01, -5.4466e-01,  4.3088e-01, -2.9210e-01,\n",
       "          1.1778e-01, -6.9603e-02,  7.5977e-02, -4.5368e-01,  1.4878e-01,\n",
       "          5.3063e-01,  3.6615e-01,  1.1995e-02,  1.3695e-01, -2.2591e-01,\n",
       "          7.9433e-04, -3.2749e-02, -2.4136e-01, -8.6491e-02,  1.0961e-01,\n",
       "          6.2512e-02, -5.2186e-01,  5.3180e-01,  1.3416e-01, -1.2086e-01,\n",
       "         -8.7871e-01,  2.7758e-01,  8.9717e-02, -1.0555e-01,  9.1845e-02,\n",
       "          4.1016e-01, -8.7682e-02,  1.1462e-01, -8.8648e-02, -7.8591e-02,\n",
       "         -2.8746e-02,  1.5249e-01, -4.4001e-01,  4.6141e-01,  3.9373e-02,\n",
       "          6.3772e-02,  7.2266e-02,  5.4953e-02, -4.9314e-02,  7.2129e-02,\n",
       "          5.0154e-02,  8.5779e-02, -8.1904e-02, -2.5208e-01, -2.8284e-01,\n",
       "          4.5718e-01, -3.6298e-01,  2.0661e-01,  2.6435e-01,  1.4899e-01,\n",
       "          3.2747e-01,  2.2340e-02, -2.6503e-02, -1.0650e-01, -3.9311e-01,\n",
       "         -1.6283e-01,  2.4327e-01, -3.2295e-01, -8.4207e-02,  5.4584e-02,\n",
       "         -5.5150e-02, -3.4717e-01, -1.5263e-01, -2.0433e-01,  2.4693e-01,\n",
       "         -2.2193e-01, -1.1637e-01, -5.4154e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 25\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 25\n",
      "Number of layers per token: 12\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "  # Holds 12 layers of hidden states for each token \n",
    "  hidden_layers = [] \n",
    "  \n",
    "  # For each of the 12 layers...\n",
    "  for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "    \n",
    "    hidden_layers.append(vec)\n",
    "    \n",
    "  token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 25 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As an alternative method, let's try creating the word vectors by summing together the last four layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 25 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\"), sentence_embedding[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, tensor([-5.1646e-02, -2.7428e-01, -4.2455e-02,  3.5630e-01,  7.8327e-01,\n",
       "          2.1895e-01, -5.0150e-02,  4.8984e-01,  6.0959e-02, -3.9689e-01,\n",
       "          6.3904e-02, -1.1114e-01,  2.5166e-02,  3.6832e-01, -4.7702e-01,\n",
       "          1.8981e-01,  2.5177e-01,  8.4108e-02,  4.9402e-01,  1.2160e-01,\n",
       "          1.0485e-01, -8.8513e-03,  1.6972e-01,  2.1172e-01,  3.5628e-01,\n",
       "          1.1330e-01, -2.2521e-01,  1.0850e-01, -2.2228e-01,  2.6852e-01,\n",
       "          5.1576e-01, -1.1650e-01,  1.5095e-01, -3.9875e-01, -1.3337e-01,\n",
       "         -4.2994e-01, -2.3136e-01, -5.1193e-02, -1.0509e-01,  2.5181e-01,\n",
       "         -3.0920e-01, -3.5429e-01, -2.4497e-01,  2.1342e-01,  5.6274e-02,\n",
       "         -4.7383e-01, -9.8232e-02, -1.3003e-01, -1.7795e-01,  6.7389e-02,\n",
       "         -3.1509e-01,  7.6815e-01, -6.1594e-01, -2.3484e-02,  1.0953e-01,\n",
       "          6.5318e-01, -3.1962e-01, -6.5844e-01, -5.4310e-02, -1.0029e-01,\n",
       "          3.4368e-01, -3.3413e-01,  3.7650e-01, -4.7105e-01,  8.7584e-02,\n",
       "          6.1833e-03,  4.1972e-01,  1.7189e-01, -3.1287e-01,  2.2347e-02,\n",
       "         -3.5401e-01,  2.3494e-01, -1.5566e-01, -4.8569e-01, -5.0042e-02,\n",
       "          3.1581e-01, -1.3727e-01,  5.2178e-02, -2.5753e-01,  2.0502e-01,\n",
       "         -1.3896e-01,  3.1224e-01, -1.5849e-01,  2.4086e-01,  9.0516e-02,\n",
       "          4.9025e-02,  2.0589e-02,  8.1054e-02, -6.2214e-02,  5.4753e-01,\n",
       "         -7.8044e-02,  5.9233e-02, -2.6315e-01,  5.6777e-03, -3.8673e-01,\n",
       "          1.4415e-01, -1.7894e-01,  7.0718e-02, -2.4774e-01,  4.0958e-01,\n",
       "         -4.7111e-02, -5.1000e-01,  1.7965e-01,  3.7998e-01,  2.0480e-02,\n",
       "          2.7902e-02,  2.7470e-01,  2.2288e-01, -2.6848e-01, -6.8985e-02,\n",
       "          1.3882e-01, -2.8467e-01,  2.4286e-01, -1.6129e-01, -9.3578e-03,\n",
       "         -4.8989e-02,  5.0700e-01, -2.9507e-01, -4.8762e-01, -4.8277e-02,\n",
       "          3.9001e-01,  3.0321e-01,  3.8065e-01,  9.2160e-01, -5.8430e-02,\n",
       "          1.8684e-01, -3.1164e-02,  9.8771e-02, -5.3407e-02, -5.3591e-01,\n",
       "          3.4570e-01,  2.9832e-02, -3.1431e-02, -1.3005e-01, -2.2007e-01,\n",
       "          2.2746e-01, -8.9087e-02, -2.5143e-01, -3.9925e-01,  2.2390e-01,\n",
       "          1.6739e-01, -2.5522e-01,  2.9284e-03,  1.8745e-01,  1.2434e-01,\n",
       "          3.2096e-01,  1.9578e-01, -2.3533e-01,  2.0619e-01,  2.8698e-01,\n",
       "          2.5212e-01,  2.7015e-01, -2.4198e-01, -2.1793e-01,  2.1776e-01,\n",
       "          2.3250e-01,  2.1671e-02,  4.2377e-02, -1.9232e-01, -2.7690e-02,\n",
       "          3.3266e-01,  3.2735e-01, -3.7334e-01,  2.8837e-01, -9.3109e-02,\n",
       "         -9.4972e-02,  1.2573e-01,  1.6729e-01, -2.0387e-01, -3.9042e-02,\n",
       "         -2.3434e-01, -2.5629e-02,  5.8811e-01,  2.4643e-02,  1.1508e-01,\n",
       "          1.1590e-01, -1.4044e-01, -7.3878e-02,  1.4789e-01, -2.8251e-02,\n",
       "         -1.0575e+00,  2.7311e-01,  2.1568e-01,  1.5133e-01,  2.6989e-01,\n",
       "         -1.9430e-01,  3.1738e-01,  1.3396e-01, -1.9212e-01, -3.1234e-01,\n",
       "          7.6941e-02, -5.1352e-01, -6.3638e-01,  9.0510e-02, -4.0561e-02,\n",
       "         -9.5713e-02, -3.0011e-01, -1.1051e-01, -7.3314e-02, -5.3624e-02,\n",
       "         -2.6177e-01, -5.4208e-02,  2.0644e-01,  7.6147e-02,  3.4010e-04,\n",
       "          2.3951e-02,  1.8490e-01, -1.9578e-02,  4.1352e-01,  1.7411e-01,\n",
       "         -2.0257e-01,  3.8622e-01,  1.6076e-01, -4.3518e-02,  1.0301e-01,\n",
       "         -1.3237e-01, -2.6903e-01, -4.7696e-01,  5.1126e-03,  1.1169e-01,\n",
       "         -2.7196e-02,  2.8770e-01, -1.0527e-01,  3.6502e-01,  1.5014e-02,\n",
       "          8.5476e-01, -2.4173e-01, -3.3721e-01, -2.1084e-01,  1.2239e-01,\n",
       "         -1.9063e-01, -1.1702e-01,  2.2121e-01, -9.7392e-02, -2.0538e-01,\n",
       "         -5.0193e-02, -2.0448e-02, -8.1061e-02, -1.7168e-01, -3.3794e-01,\n",
       "         -5.8501e-02,  1.2916e-01,  4.8006e-01, -1.0564e-01,  1.9816e-01,\n",
       "         -3.2302e-02,  2.5870e-01,  5.2942e-01, -1.1898e-01, -2.6245e-01,\n",
       "         -3.7280e-01, -2.4396e-01, -2.9742e-01, -4.2863e-01,  1.9494e-01,\n",
       "         -1.7290e-01, -1.0845e-01, -7.5270e-02,  4.4981e-03,  1.7508e-01,\n",
       "          3.8252e-01, -8.7430e-02, -2.6883e-02, -4.5373e-02, -1.9720e-01,\n",
       "         -3.1361e-01, -3.0977e-01,  2.4946e-01,  3.1379e-01,  2.2730e-01,\n",
       "          8.4108e-02,  1.3018e-01,  4.0233e-02,  4.3390e-01,  1.1528e-01,\n",
       "         -1.8460e-01,  9.4047e-02,  7.8173e-02,  8.8990e-02, -1.2832e-01,\n",
       "          1.3446e-01,  3.8816e-01, -3.1608e-01, -3.2205e-01,  1.1504e-01,\n",
       "         -3.0539e-01,  4.2782e-02,  5.4093e-01, -2.2912e-01, -2.0253e-01,\n",
       "         -4.7722e-02,  1.7305e-01, -3.5210e-01,  2.0325e-01, -1.2559e-01,\n",
       "          2.5857e-01,  3.7477e-03,  6.5955e-01,  5.9500e-01, -4.0141e-02,\n",
       "          3.3596e-03,  9.1899e-02,  3.7765e-01, -3.4921e-02, -9.0647e-02,\n",
       "          4.0187e-02,  4.6837e-01, -2.9089e-01, -4.2177e+00,  6.2771e-01,\n",
       "          3.8050e-01, -3.3698e-01,  1.2023e-01, -1.2394e-01, -2.1404e-01,\n",
       "         -4.2955e-01, -2.7179e-01,  9.1991e-02, -2.3187e-01, -5.7365e-01,\n",
       "          4.3775e-01, -1.3180e-01,  1.0445e-01, -3.9577e-01,  2.4761e-01,\n",
       "         -2.0251e-01,  6.3868e-02,  1.2256e-01, -2.7493e-01, -2.1371e-01,\n",
       "         -2.2803e-01,  8.0862e-02,  4.0949e-01,  3.6303e-01, -6.2829e-01,\n",
       "          5.3444e-02,  1.2802e-01,  6.8184e-02,  1.0805e-01, -2.6069e-01,\n",
       "         -2.0091e-01, -3.7778e-01,  2.0808e-01,  6.4645e-02, -3.1055e-02,\n",
       "         -7.0195e-02,  1.1171e-01,  1.6970e-01, -2.2638e-01, -4.0377e-01,\n",
       "         -1.7460e-01, -2.4194e-02,  9.4511e-01,  1.8343e-01, -5.2386e-02,\n",
       "         -5.5351e-01,  8.7157e-02,  1.8157e-01,  4.1054e-01, -3.7095e-01,\n",
       "         -9.3232e-02,  2.8381e-01,  2.0373e-01, -7.6650e-04,  2.3849e-01,\n",
       "          5.9567e-02, -1.9520e-01, -1.5530e-01, -1.1363e-01, -4.1650e-01,\n",
       "         -2.0237e-01,  1.7195e-01, -5.3862e-02, -3.2323e-01, -3.3564e-01,\n",
       "          3.9400e-02,  2.1783e-01,  6.4307e-02, -5.3462e-02,  3.0733e-01,\n",
       "         -3.8965e-01, -8.5370e-01, -3.3795e-01,  3.9803e-02,  5.5881e-01,\n",
       "         -2.4414e-01,  2.1048e-01, -1.4443e-01, -3.2982e-01, -1.0111e-01,\n",
       "          1.7442e-01,  3.0266e-02,  3.0276e-02, -3.3739e-01,  3.0244e-02,\n",
       "          2.2977e-01, -3.7677e-01, -5.3196e-01,  2.5440e-01,  2.9221e-01,\n",
       "          1.7142e-01, -5.1496e-02,  8.2988e-02,  5.3693e-02, -1.9968e-01,\n",
       "         -2.8214e-01,  5.1436e-02,  1.8023e-02,  1.6014e-01, -1.1766e-01,\n",
       "          3.7324e-01, -2.7330e-01, -6.4651e-02, -1.1297e-01, -2.7177e-01,\n",
       "          7.5423e-02, -5.5792e-01,  2.2879e-01,  2.4242e-01, -6.3162e-01,\n",
       "          6.6050e-01,  2.8139e-02,  1.1126e-01,  2.5066e-01,  5.6591e-01,\n",
       "          3.4117e-01,  1.4173e-01,  4.6219e-02, -3.2003e-01,  5.5092e-01,\n",
       "         -4.1561e-01, -3.6245e-01,  2.2650e-02, -1.2092e-01,  3.5361e-02,\n",
       "         -6.0898e-02,  2.7024e-03, -4.2688e-01,  9.3343e-02, -3.5025e-01,\n",
       "          1.6733e-01,  1.0473e-01,  4.7048e-01, -1.6659e-01,  2.3450e-01,\n",
       "         -9.5543e-01,  7.5668e-02,  2.4800e-01, -1.2310e-01,  3.5721e-03,\n",
       "          9.3809e-02, -1.7120e-01, -2.2369e-01,  1.3177e-01,  2.0100e-02,\n",
       "         -8.7770e-02, -9.1973e-02, -7.6939e-02, -3.2589e-01, -9.7160e-02,\n",
       "          1.3470e-01, -4.3150e-01, -1.5666e-01,  3.7806e-01,  2.3398e-01,\n",
       "         -1.5443e-01, -4.9362e-01, -8.5777e-02,  1.7316e-01,  2.0668e-01,\n",
       "          3.8173e-01,  7.8196e-02, -9.1806e-01,  2.1363e-01, -4.3684e-01,\n",
       "         -2.0484e-01, -2.5964e-01,  5.7678e-01,  1.4474e-02, -2.4837e-01,\n",
       "          3.9330e-02,  1.5871e-01, -2.7883e-02,  3.6258e-01,  1.4813e-01,\n",
       "         -3.2574e-01, -1.1970e-01,  8.1526e-02, -2.8232e-01,  1.2616e-01,\n",
       "         -7.4043e-02, -2.5993e-01,  3.2443e-01, -8.7091e-02, -4.1982e-02,\n",
       "          6.5980e-02,  1.0437e-01,  6.4116e-02,  5.8662e-02,  3.4397e-01,\n",
       "         -3.8440e-02,  1.5037e-01, -3.2674e-01, -2.7289e-01,  1.3577e-01,\n",
       "          2.2445e-01,  2.5487e-02,  3.3640e-01,  1.6794e-01,  1.1338e-02,\n",
       "         -2.6774e-01, -6.9100e-02, -1.0064e-01, -3.2657e-01,  2.5673e-01,\n",
       "          2.8813e-01, -3.4367e-01,  2.7603e-01, -2.2375e-01, -1.2103e-02,\n",
       "         -6.3439e-01, -5.8248e-01,  3.6660e-02, -1.0049e-01,  4.3203e-01,\n",
       "          1.5304e-01, -2.5209e-01, -7.6161e-02, -1.4671e-01, -6.4912e-02,\n",
       "          1.6623e-01, -4.6252e-02, -5.0993e-01, -1.6316e-01,  2.4132e-01,\n",
       "          1.2427e-01,  2.4708e-02,  1.3280e-01,  1.5665e-02, -7.9885e-02,\n",
       "         -4.7467e-03,  1.7861e-01,  3.4154e-01,  1.0195e-01, -1.2637e-01,\n",
       "         -6.3952e-01, -1.9848e-01, -1.8358e-01, -1.0386e-01,  2.5660e-01,\n",
       "         -1.7948e-01, -2.5389e-02,  2.6166e-01,  3.0825e-01,  2.0378e-01,\n",
       "          3.0332e-01,  1.9788e-02,  2.9216e-01,  2.3874e-02, -8.4977e-02,\n",
       "          1.9587e-01,  1.2738e-01, -4.2802e-01, -1.8932e-01, -3.5191e-01,\n",
       "          7.7653e-02, -4.6272e-02,  3.4260e-01,  2.4034e-02, -4.0628e-01,\n",
       "         -3.0562e-01,  2.3529e-01,  4.3604e-01,  3.6333e-02,  1.1093e-01,\n",
       "          3.4855e-01,  6.6475e-02, -1.6496e-01,  1.3166e-01, -4.6289e-01,\n",
       "          7.3864e-02,  1.1152e-02, -2.3522e-01,  2.9151e-01,  8.5543e-03,\n",
       "         -6.8325e-01, -2.4929e-01, -3.9269e-01, -4.0800e-01,  8.9593e-02,\n",
       "          1.5912e-01, -4.6886e-02,  4.5136e-02, -1.1893e-01, -4.4697e-03,\n",
       "          2.2628e-01, -1.1403e-01, -7.3683e-01,  1.6632e-01,  4.0062e-02,\n",
       "          1.9401e-01, -3.4868e-02, -1.7646e-01,  2.5105e-01,  4.7340e-01,\n",
       "          1.6331e-01,  3.0211e-01,  3.3110e-01, -9.8451e-02,  1.0899e-01,\n",
       "         -3.2928e-03,  2.5364e-01, -1.7989e-01,  3.1227e-01, -1.8503e-01,\n",
       "         -2.3737e-01, -1.4714e-01,  1.3387e-01, -7.4007e-02, -6.2870e-01,\n",
       "          3.4302e-01,  6.8974e-02, -3.2966e-01, -2.4581e-01,  2.3443e-02,\n",
       "         -1.6544e-01, -6.8264e-01,  1.4906e-01,  7.2938e-02, -4.3300e-02,\n",
       "          3.5407e-01,  2.7247e-01,  1.9614e-01,  3.7570e-01, -2.1177e-01,\n",
       "          5.3069e-02, -1.0399e-01,  3.1998e-01, -2.6074e-01,  2.7843e-02,\n",
       "         -2.2418e-01,  2.4131e-01, -1.9104e-01, -5.6782e-02,  7.0549e-02,\n",
       "         -4.1843e-01,  2.9671e-01, -2.6210e-01,  2.5369e-02, -1.1147e-01,\n",
       "          3.1548e-02,  7.1527e-01,  2.6561e-01, -2.9342e-02, -4.2071e-02,\n",
       "         -6.7688e-02,  4.0806e-01,  4.3499e-01,  6.5205e-02,  7.6585e-02,\n",
       "         -1.1925e-01, -5.9912e-01,  6.2302e-01,  6.7492e-02,  2.7255e-01,\n",
       "         -6.0694e-02, -3.4032e-01,  3.8022e-01,  2.2814e-01, -2.0414e-01,\n",
       "          4.7198e-01, -4.1908e-01, -1.8191e-01,  1.1152e-01, -3.7447e-01,\n",
       "         -3.1561e-01, -1.4297e-02, -1.1335e-01,  2.6851e-01,  2.8706e-01,\n",
       "         -2.9540e-01, -3.3900e-01, -7.6403e-02,  3.5745e-01,  7.6169e-03,\n",
       "          2.9281e-01, -2.0142e-02, -1.1438e-01, -1.8556e-01, -2.9250e-01,\n",
       "         -4.5004e-01, -4.7123e-01, -2.8028e-01, -6.6309e-02, -3.3889e-02,\n",
       "         -1.4704e-02, -1.0398e-01, -5.4466e-01,  4.3088e-01, -2.9210e-01,\n",
       "          1.1778e-01, -6.9603e-02,  7.5977e-02, -4.5368e-01,  1.4878e-01,\n",
       "          5.3063e-01,  3.6615e-01,  1.1995e-02,  1.3695e-01, -2.2591e-01,\n",
       "          7.9433e-04, -3.2749e-02, -2.4136e-01, -8.6491e-02,  1.0961e-01,\n",
       "          6.2512e-02, -5.2186e-01,  5.3180e-01,  1.3416e-01, -1.2086e-01,\n",
       "         -8.7871e-01,  2.7758e-01,  8.9717e-02, -1.0555e-01,  9.1845e-02,\n",
       "          4.1016e-01, -8.7682e-02,  1.1462e-01, -8.8648e-02, -7.8591e-02,\n",
       "         -2.8746e-02,  1.5249e-01, -4.4001e-01,  4.6141e-01,  3.9373e-02,\n",
       "          6.3772e-02,  7.2266e-02,  5.4953e-02, -4.9314e-02,  7.2129e-02,\n",
       "          5.0154e-02,  8.5779e-02, -8.1904e-02, -2.5208e-01, -2.8284e-01,\n",
       "          4.5718e-01, -3.6298e-01,  2.0661e-01,  2.6435e-01,  1.4899e-01,\n",
       "          3.2747e-01,  2.2340e-02, -2.6503e-02, -1.0650e-01, -3.9311e-01,\n",
       "         -1.6283e-01,  2.4327e-01, -3.2295e-01, -8.4207e-02,  5.4584e-02,\n",
       "         -5.5150e-02, -3.4717e-01, -1.5263e-01, -2.0433e-01,  2.4693e-01,\n",
       "         -2.2193e-01, -1.1637e-01, -5.4154e-02]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\"), sentence_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import spacy\n",
    "\n",
    "# import gui\n",
    "\n",
    "import re\n",
    "\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    df = pd.read_csv(path,\n",
    "                     sep='\\t',\n",
    "                     header=0\n",
    "                     )\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sent_emd(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "    return torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "\n",
    "def jaccard_query_similarity(doc_path, query_path):\n",
    "    df_query = pd.read_csv(query_path,\n",
    "                     sep='\\t',\n",
    "                     header=0\n",
    "                     )\n",
    "\n",
    "    df = read_file(doc_path)\n",
    "\n",
    "    jac_sim = list()\n",
    "    head_w = 0.6\n",
    "    cont_w = 1 - head_w\n",
    "\n",
    "    # theme = df_query.iloc[0]['TAGS']\n",
    "    # df_theme = df[df['TAGS'] == theme]\n",
    "    # df_theme = df_theme.dropna()\n",
    "    # df_theme = df_theme.reset_index(drop = True)\n",
    "\n",
    "    # df_size = len(df_theme.index)\n",
    "    df_size = len(df.index)\n",
    "\n",
    "\n",
    "\n",
    "    for sl in range(df_size):\n",
    "        # h_sim = jaccard_similarity(df_query.iloc[0]['TITLE'], df_theme.iloc[sl]['TITLE'])\n",
    "        # c_sim = jaccard_similarity(df_query.iloc[0]['MAIN_CONTENT'], df_theme.iloc[sl]['MAIN_CONTENT'])\n",
    "        h_sim = jaccard_similarity(df_query.iloc[0]['TITLE'], df.iloc[sl]['TITLE'])\n",
    "        c_sim = jaccard_similarity(df_query.iloc[0]['MAIN_CONTENT'], df.iloc[sl]['MAIN_CONTENT'])\n",
    "        q_w = head_w*h_sim + cont_w*c_sim\n",
    "        jac_sim.append(q_w)\n",
    "\n",
    "    # import operator\n",
    "    # index, value = max(enumerate(jac_sim), key=operator.itemgetter(1))\n",
    "    # similar_query = df_theme.loc[[index]]\n",
    "\n",
    "    jac_sim = np.asarray(jac_sim)\n",
    "    indx = jac_sim.argsort()[-5:][::-1]\n",
    "\n",
    "    # print(\"#######################################################################################################\")\n",
    "    # print(\"JACCARD SIMILARITY\")\n",
    "    # print(\"#######################################################################################################\")\n",
    "    # print(\"jaccard Similarity: {}\".format(value))\n",
    "    # print(\"Similar query:\\n\", similar_query)\n",
    "    # print(\"#######################################################################################################\")\n",
    "\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"JACCARD SIMILARITY\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"Top 5 Similar queries:\\n\")\n",
    "    print(\" ID               SUMMARY                      QUERY                            \"\n",
    "          \"    THEME                   SIMILARITY\")\n",
    "    for temp, i in enumerate(indx):\n",
    "        print(df.loc[[i]].to_string(header=False, index=False), jac_sim[indx[temp]])\n",
    "    print(\"#######################################################################################################\")\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# doc_l = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity_1(spacy_model, query, document):\n",
    "    doc1 = spacy_model(query)\n",
    "    doc2 = spacy_model(document)\n",
    "    return doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_query_similarity(doc_path, query_path):\n",
    "    df_query = pd.read_csv(query_path,\n",
    "                           sep='\\t',\n",
    "                           header=0\n",
    "                           )\n",
    "\n",
    "    df = read_file(doc_path)\n",
    "\n",
    "    cos_sim = list()\n",
    "    headc_w = 0.6\n",
    "    contc_w = 1 - headc_w\n",
    "\n",
    "    # theme = df_query.iloc[0]['TAGS']\n",
    "    # df_theme_c = df[df['TAGS'] == theme]\n",
    "    # df_theme_c = df_theme_c.dropna()\n",
    "    # df_theme_c = df_theme_c.reset_index(drop=True)\n",
    "\n",
    "    # df_size = len(df_theme_c.index)\n",
    "    df_size = len(df.index)\n",
    "\n",
    "\n",
    "    nlp_e = spacy.load(r\"spacy.word2vec.model\")\n",
    "\n",
    "    for ind in range(df_size):\n",
    "        # h_sim = cosine_similarity(nlp_e, df_query.iloc[0]['TITLE'], df_theme_c.iloc[ind]['TITLE'])\n",
    "        # c_sim = cosine_similarity(nlp_e, df_query.iloc[0]['MAIN_CONTENT'], df_theme_c.iloc[ind]['MAIN_CONTENT'])\n",
    "        h_sim = cosine_similarity_1(nlp_e,df_query.iloc[0]['TITLE'], df.iloc[ind]['TITLE'])\n",
    "        c_sim = cosine_similarity_1(nlp_e, df_query.iloc[0]['MAIN_CONTENT'], df.iloc[ind]['MAIN_CONTENT'])\n",
    "        q_w = headc_w*h_sim + contc_w*c_sim\n",
    "        cos_sim.append(q_w)\n",
    "\n",
    "    # import operator\n",
    "    # index, value = max(enumerate(cos_sim), key=operator.itemgetter(1))\n",
    "\n",
    "    cos_sim = np.asarray(cos_sim)\n",
    "    indx = cos_sim.argsort()[-5:][::-1]\n",
    "\n",
    "    # print(indx)\n",
    "    # print(cos_sim[indx])\n",
    "\n",
    "\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"COSINE SIMILARITY\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"Top 5 Similar queries:\\n\")\n",
    "    print(\" ID               SUMMARY                      QUERY                            \"\n",
    "          \"    THEME                   SIMILARITY\")\n",
    "    for temp, i in enumerate(indx):\n",
    "        print(df.loc[[i]].to_string(header=False, index=False), cos_sim[indx[temp]])\n",
    "    print(\"#######################################################################################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_query__bert_similarity(doc_path, query_path):\n",
    "    df_query = pd.read_csv(query_path,\n",
    "                           sep='\\t',\n",
    "                           header=0\n",
    "                           )\n",
    "\n",
    "    df = read_file(doc_path)\n",
    "\n",
    "    cos_sim = list()\n",
    "    headc_w = 0.6\n",
    "    contc_w = 1 - headc_w\n",
    "\n",
    "    # theme = df_query.iloc[0]['TAGS']\n",
    "    # df_theme_c = df[df['TAGS'] == theme]\n",
    "    # df_theme_c = df_theme_c.dropna()\n",
    "    # df_theme_c = df_theme_c.reset_index(drop=True)\n",
    "\n",
    "    # df_size = len(df_theme_c.index)\n",
    "    df_size = len(df.index)\n",
    "\n",
    "\n",
    "#     nlp_e = spacy.load(r\"spacy.word2vec.model\")\n",
    "\n",
    "    for ind in range(df_size):\n",
    "#         print(ind)\n",
    "        # h_sim = cosine_similarity(nlp_e, df_query.iloc[0]['TITLE'], df_theme_c.iloc[ind]['TITLE'])\n",
    "        # c_sim = cosine_similarity(nlp_e, df_query.iloc[0]['MAIN_CONTENT'], df_theme_c.iloc[ind]['MAIN_CONTENT'])\n",
    "#         c = bert_sent_emd(df_query.iloc[0]['TITLE']).shape\n",
    "#         d = bert_sent_emd(df_query.iloc[ind]['TITLE']).shape\n",
    "        \n",
    "#         print(c, d)\n",
    "        h_sim = cosine_similarity(bert_sent_emd(df_query.iloc[0]['TITLE']).reshape(1,-1), bert_sent_emd(df.iloc[ind]['TITLE']).reshape(1,-1))[0][0]\n",
    "#         print(h_sim)\n",
    "#         h_sim = cosine_similarity(nlp_e,df_query.iloc[0]['TITLE'], df.iloc[ind]['TITLE'])\n",
    "        c_sim = cosine_similarity(bert_sent_emd(df_query.iloc[0]['MAIN_CONTENT']).reshape(1,-1), bert_sent_emd(df.iloc[ind]['MAIN_CONTENT']).reshape(1,-1))[0][0]\n",
    "#         c_sim = cosine_similarity(nlp_e, df_query.iloc[0]['MAIN_CONTENT'], df.iloc[ind]['MAIN_CONTENT'])\n",
    "        q_w = headc_w*h_sim + contc_w*c_sim\n",
    "        cos_sim.append(q_w)\n",
    "\n",
    "    # import operator\n",
    "    # index, value = max(enumerate(cos_sim), key=operator.itemgetter(1))\n",
    "\n",
    "    cos_sim = np.asarray(cos_sim)\n",
    "    indx = cos_sim.argsort()[-5:][::-1]\n",
    "\n",
    "    # print(indx)\n",
    "    # print(cos_sim[indx])\n",
    "\n",
    "\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"COSINE SIMILARITY BERT\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"Top 5 Similar queries:\\n\")\n",
    "    print(\" ID               SUMMARY                      QUERY                            \"\n",
    "          \"    THEME                   SIMILARITY\")\n",
    "    for temp, i in enumerate(indx):\n",
    "        print(df.loc[[i]].to_string(header=False, index=False), cos_sim[indx[temp]])\n",
    "    print(\"#######################################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################################################################\n",
      "COSINE SIMILARITY\n",
      "#######################################################################################################\n",
      "Top 5 Similar queries:\n",
      "\n",
      " ID               SUMMARY                      QUERY                                THEME                   SIMILARITY\n",
      " jc7ohpgq60i6kk  opencv cmake error  make build folder write follow command sudo cm...  planter_bot 0.999773990071659\n",
      " jbro40nu6zk22w  error opencv installation  follwe step perfectly get opencv installation  planter_bot 0.9997725912110422\n",
      " jcop2x2n7zj4h  error opencv  cv imshow work asking rebuild opencv undergoing  planter_bot 0.9997551989355732\n",
      " jd8dyl1ypbrer  error  camera work fine suddenly show error tell erro...  planter_bot 0.9997258517400738\n",
      " jag8lqtnqp65pa  error  get indent error output file get output run fi...  harvester_bot 0.9996786248019801\n",
      "#######################################################################################################\n",
      "\n",
      "\n",
      "#######################################################################################################\n",
      "JACCARD SIMILARITY\n",
      "#######################################################################################################\n",
      "Top 5 Similar queries:\n",
      "\n",
      " ID               SUMMARY                      QUERY                                THEME                   SIMILARITY\n",
      " jbro40nu6zk22w  error opencv installation  follwe step perfectly get opencv installation  planter_bot 0.8857142857142857\n",
      " jbo54uwbn9p5m7   instal opencv  face instal open cv resolving  planter_bot 0.8338461538461539\n",
      " jbjbn37xikc3uf  opencv installation rpi  execute sudo cmake -dcmake_build_type release ...  planter_bot 0.8266666666666667\n",
      " jbqlwztsj1j6pt  opencv installation  installation opencv error occur make cmake fil...  planter_bot 0.8265734265734266\n",
      " jbzzszosjq60z  opencv installation  get permission deny run cmake command command ...  planter_bot 0.7692307692307693\n",
      "#######################################################################################################\n",
      "\n",
      "#######################################################################################################\n",
      "COSINE SIMILARITY BERT\n",
      "#######################################################################################################\n",
      "Top 5 Similar queries:\n",
      "\n",
      " ID               SUMMARY                      QUERY                                THEME                   SIMILARITY\n",
      " jbro40nu6zk22w  error opencv installation  follwe step perfectly get opencv installation  planter_bot 0.8651904702186584\n",
      " jbqlwztsj1j6pt  opencv installation  installation opencv error occur make cmake fil...  planter_bot 0.8425062537193299\n",
      " jcop2x2n7zj4h  error opencv  cv imshow work asking rebuild opencv undergoing  planter_bot 0.8403941631317138\n",
      " jc7ohpgq60i6kk  opencv cmake error  make build folder write follow command sudo cm...  planter_bot 0.8332557439804077\n",
      " jbjbn37xikc3uf  opencv installation rpi  execute sudo cmake -dcmake_build_type release ...  planter_bot 0.8261630058288574\n",
      "#######################################################################################################\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "\n",
    "    # desired_width = 1000\n",
    "    # pd.set_option('display.width', desired_width)\n",
    "    # pd.set_option('display.max_columns', 10)\n",
    "    # \n",
    "    # app = gui.Post()\n",
    "    # app.run()\n",
    "\n",
    "    cosine_query_similarity(\"dataset/PIAZZA_clean_single_theme.tsv\", \"query.tsv\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    jaccard_query_similarity(\"dataset/PIAZZA_clean_single_theme.tsv\", \"query.tsv\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    cosine_query__bert_similarity(\"dataset/PIAZZA_clean_single_theme.tsv\", \"query.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
